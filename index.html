<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Marzieh  Fadaee</title>
    <meta name="author" content="Marzieh  Fadaee">
    <meta name="description" content="Homepage for Marzieh Fadaee
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marziehf.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Marzieh</span>  Fadaee
          </h1>
          <p class="desc"><strong>Senior research scientist</strong> @ <a href="https://cohere.for.ai/" rel="external nofollow noopener" target="_blank">Cohere For AI</a></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_moi-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_moi-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_moi-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic_moi.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic_moi.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>As a scientist, I’m broadly interested in all aspects of natural language understanding, and particularly in multilingual learning, data-conscious learning, robust and scalable models, compositionality, and interpretability.</p>

<p>Previously I was the NLP/ML research lead at <a href="https://www.zeta-alpha.com/" rel="external nofollow noopener" target="_blank">Zeta Alpha Vector</a> working on smarter ways to discover and organize knowledge in AI. 
I did my PhD at the <a href="https://ltl.science.uva.nl/" rel="external nofollow noopener" target="_blank">Language Technology Lab</a> (originally part of the <a href="https://irlab.science.uva.nl/" rel="external nofollow noopener" target="_blank">ILPS group</a>), University of Amsterdam, working on developing models to understand and utilize interesting phenomena in the data. 
During my PhD I was advised by <a href="https://staff.fnwi.uva.nl/c.monz/index.html" rel="external nofollow noopener" target="_blank">Christof Monz</a> and <a href="http://www.cs.rug.nl/~bisazza/index.html" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>. I received my B.Sc. from <a href="http://www.en.sharif.edu/" rel="external nofollow noopener" target="_blank">Sharif University</a> majoring in Computer Engineering and M.Sc. from <a href="https://ut.ac.ir/en" rel="external nofollow noopener" target="_blank">University of Tehran</a> majoring in Artificial Intelligence.</p>


          </div>

          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Jan 20, 2023</th>
                  <td>
                    I’m excited to announce that I have joined Sara Hooker’s team at <a href="https://cohere.for.ai/" rel="external nofollow noopener" target="_blank">Cohere For AI</a> as a Senior Research Scientist <img class="emoji" title=":purple_heart:" alt=":purple_heart:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f49c.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan 12, 2023</th>
                  <td>
                    InPars-v2 is SoTA on the <a href="https://eval.ai/web/challenges/challenge-page/1897/leaderboard/4475" target="blank" rel="external nofollow noopener">BEIR Leaderboard</a> in zero-shot Information Retrieval <img class="emoji" title=":stars:" alt=":stars:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f320.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan 4, 2023</th>
                  <td>
                    Our paper <a href="https://arxiv.org/abs/2301.01820" target="blank" rel="external nofollow noopener">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</a> is available on arxiv now.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Dec 12, 2022</th>
                  <td>
                    Our paper <a href="https://arxiv.org/abs/2212.06121" target="blank" rel="external nofollow noopener">In Defense of Cross-Encoders for Zero-Shot Retrieval</a> is available on arxiv now.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb 10, 2022</th>
                  <td>
                    Our paper <a href="https://arxiv.org/abs/2202.05144" target="blank" rel="external nofollow noopener">InPars: Data Augmentation for Information Retrieval using Large Language Models</a> got accepted at SIGIR.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan 1, 2022</th>
                  <td>
                    Our paper <a href="https://arxiv.org/abs/2108.13897" target="blank" rel="external nofollow noopener">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset</a> is available on arxiv now.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Sep 10, 2021</th>
                  <td>
                    Invited speaker at “Transformers at Work: 2nd edition” workshop.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Nov 10, 2020</th>
                  <td>
                    I successfully defended my PhD dissertation! Check out my book <a href="https://marziehf.github.io/assets/pdf/Thesis.pdf" target="blank">here</a> <img class="emoji" title=":book:" alt=":book:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">May 20, 2020</th>
                  <td>
                    Our paper <a href="https://arxiv.org/abs/2005.12398" target="blank" rel="external nofollow noopener">The Unreasonable Volatility of Neural Machine Translation Models</a> got accepted at WNGT. It will be presented at ACL.
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Jan 17, 2020</th>
                  <td>
                    Invited speaker at “Transformers at Work” workshop.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/v2.png"></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2301.01820" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval</div>
          <!-- Author -->
          <div class="author">
          

          Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, Jakub Zavrel, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/inpars.png"></div>

        <!-- Entry bib key -->
        <div id="bonifacio2022inpars" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">InPars: Data Augmentation for Information Retrieval using Large Language Models</div>
          <!-- Author -->
          <div class="author">
          

          Luiz Henrique Bonifacio, Hugo Abonizio, <em>Marzieh Fadaee</em>, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In SIGIR</em> Feb 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2202.05144" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The information retrieval community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our unsupervised dataset outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Furthermore, retrievers finetuned on both supervised and our synthetic data achieve better zero-shot transfer than models finetuned only on supervised data. Code, models, and data are available at https://github.com/zetaalphavector/inpars</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/noparam.png"></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2206.02873" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval</div>
          <!-- Author -->
          <div class="author">
          

          Guilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, <em>Marzieh Fadaee</em>, Roberto Lotufo, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In arXiv</em> Feb 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2206.02873" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent work has shown that small distilled language models are strong competitors to models that are orders of magnitude larger and slower in a wide range of information retrieval tasks. This has made distilled and dense models, due to latency constraints, the go-to choice for deployment in real-world retrieval applications. In this work, we question this practice by showing that the number of parameters and early query-document interaction play a significant role in the generalization ability of retrieval models. Our experiments show that increasing model size results in marginal gains on in-domain test sets, but much larger gains in new domains never seen during fine-tuning. Furthermore, we show that rerankers largely outperform dense ones of similar size in several tasks. Our largest reranker reaches the state of the art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the previous state of the art by 3 average points. Finally, we confirm that in-domain effectiveness is not a good indicator of zero-shot effectiveness. Code is available at this https URL</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/mmarco.png"></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2108.13897" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset</div>
          <!-- Author -->
          <div class="author">
          

          Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, <em>Marzieh Fadaee</em>, Roberto Lotufo, and <a href="https://sites.google.com/site/rodrigofrassettonogueira/" rel="external nofollow noopener" target="_blank">Rodrigo Nogueira</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In arXiv</em> Feb 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2108.13897" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by fine-tuning monolingual and multilingual re-ranking models, as well as a dense multilingual model on this dataset. Experimental results demonstrate that multilingual models fine-tuned on our translated dataset achieve superior effectiveness to models fine-tuned on the original English version alone. Our distilled multilingual re-ranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and fine-tuned models are available at link.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/final_cover.png"></div>

        <!-- Entry bib key -->
        <div id="mybook" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Understanding and Enhancing the Use of Context for Machine Translation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Marzieh Fadaee</em>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            Oct 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://marziehf.github.io/assets/pdf/Thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural networks learn patterns from data to solve complex problems. To understand and infer meaning in language, neural models have to learn complicated nuances. Meaning is often determined from context. With context, languages allow meaning to be conveyed even when the specific words used are not known by the reader. To model this learning process, a system has to learn from a few instances in context and be able to generalize well to unseen cases. In this thesis, we focus on understanding certain potentials of contexts in neural models and design augmentation models to benefit from them. We focus on machine translation as an important instance of the more general language understanding problem. This task accentuates the value of capturing nuances of language and the necessity of generalization from few observations. The main problem we study in this thesis is what neural machine translation models learn from data and how we can devise more focused contexts to enhance this learning. Looking more in-depth into the role of context and the impact of data on learning models is essential to advance the Natural Language Processing (NLP) field. Understanding the importance of data in the learning process and how neural network models interact with and benefit from data can help develop more accurate NLP systems. Moreover, it helps highlight the vulnerabilities of current neural networks and provides insights into designing more robust models.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/vol.png"></div>

        <!-- Entry bib key -->
        <div id="fadaee-monz-2020-unreasonable" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">The Unreasonable Volatility of Neural Machine Translation Models</div>
          <!-- Author -->
          <div class="author">
          

          <em>Marzieh Fadaee</em>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the Fourth Workshop on Neural Generation and Translation</em> Jul 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://www.aclweb.org/anthology/2020.ngt-1.10.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where \textitunexpected changes happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26% and 19% of sentence variations, respectively.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/bt.png"></div>

        <!-- Entry bib key -->
        <div id="D18-1040" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Marzieh Fadaee</em>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> Jul 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://aclweb.org/anthology/D18-1040.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tda.png"></div>

        <!-- Entry bib key -->
        <div id="fadaee-bisazza-monz:2017:Short2" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Data Augmentation for Low-Resource Neural Machine Translation</div>
          <!-- Author -->
          <div class="author">
          

          <em>Marzieh Fadaee</em>, <a href="http://www.cs.rug.nl/~bisazza/" rel="external nofollow noopener" target="_blank">Arianna Bisazza</a>, and <a href="https://staff.science.uva.nl/c.monz/" rel="external nofollow noopener" target="_blank">Christof Monz</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</em> Jul 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://aclweb.org/anthology/P17-2090.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%6D%61%72%7A%69%65%68.%66@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=NZqs0toAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/marziehf" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/marzieh-fadaee-b7393370" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a rel="me" href="https://sigmoid.social/@marzieh" title="Mastodon" target="_blank"><i class="fab fa-mastodon"></i></a>
            

            </div>

            <div class="contact-note">
              You can even add a little note about which of these is the best way to reach you.

            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Marzieh  Fadaee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
